{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65bcca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cb6429",
   "metadata": {},
   "source": [
    " CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b66c2305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 12\n",
    "NUM_CLASSES = 2  # Binary classification: 0 (Non-defective) and 1 (Defective)\n",
    "DATASET_PATH = \"dataset\"  # üî¥ folder with class subfolders (0/ and 1/)\n",
    "MODEL_SAVE_PATH = \"cnn_pipeline_model.pth\"\n",
    "# üî¥ Set IMAGE_DIR if using CSV-based loading (path to folder containing images)\n",
    "IMAGE_DIR = None  # Example: r\"C:\\Users\\maila\\Desktop\\Defect_Detection\\Normalised_Image_256\"\n",
    "CSV_PATH = \"train_clean.csv\"  # CSV file with ID and label columns\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b59e11",
   "metadata": {},
   "source": [
    "PREPROCESSING TRANSFORMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b64be06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocessing pipeline: Resize + Normalize (equivalent to TensorFlow's Resizing + Rescaling)\n",
    "preprocessing_transform = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToTensor(),  # Converts to [0, 1] range (equivalent to Rescaling 1./255)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3858106b",
   "metadata": {},
   "source": [
    "DATASET CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3987456b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderDataset(Dataset):\n",
    "    \"\"\"Dataset for loading images from folder structure (class_0/, class_1/, etc.)\"\"\"\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Load images from subdirectories\n",
    "        for class_name in sorted(os.listdir(root_dir)):\n",
    "            class_path = os.path.join(root_dir, class_name)\n",
    "            if os.path.isdir(class_path):\n",
    "                class_label = int(class_name)  # Assuming folder names are \"0\", \"1\", etc.\n",
    "                for img_name in os.listdir(class_path):\n",
    "                    if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                        img_path = os.path.join(class_path, img_name)\n",
    "                        self.images.append(img_path)\n",
    "                        self.labels.append(class_label)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "class CSVImageDataset(Dataset):\n",
    "    \"\"\"Dataset for loading images from CSV file (ID, label) and image directory\"\"\"\n",
    "    def __init__(self, csv_path, image_dir, transform=None):\n",
    "        self.df = pd.read_csv(csv_path, dtype={\"ID\": str})\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.df.iloc[idx][\"ID\"]\n",
    "        label = int(self.df.iloc[idx][\"label\"])\n",
    "        \n",
    "        # Try to find image with different extensions\n",
    "        img_path = None\n",
    "        for ext in (\".jpg\", \".jpeg\", \".png\"):\n",
    "            candidate_path = os.path.join(self.image_dir, img_id + ext)\n",
    "            if os.path.exists(candidate_path):\n",
    "                img_path = candidate_path\n",
    "                break\n",
    "        \n",
    "        if img_path is None:\n",
    "            raise FileNotFoundError(f\"Image not found for ID: {img_id}\")\n",
    "        \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef6fc68",
   "metadata": {},
   "source": [
    "CNN MODEL WITH PREPROCESSING PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c84f80af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PreprocessingLayer(nn.Module):\n",
    "    \"\"\"Preprocessing layer that can be part of the model\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # In PyTorch, we'll apply resize and normalization in forward pass\n",
    "        # This is a wrapper to include preprocessing in the model pipeline\n",
    "        self.resize = transforms.Resize(IMG_SIZE)\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x is expected to be a PIL Image or batch of PIL Images\n",
    "        if isinstance(x, Image.Image):\n",
    "            x = self.resize(x)\n",
    "            x = self.to_tensor(x)\n",
    "            x = x.unsqueeze(0)  # Add batch dimension\n",
    "        return x\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    \"\"\"CNN Model with preprocessing included in the pipeline\"\"\"\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # CNN layers (matching TensorFlow architecture)\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        \n",
    "        # Calculate flattened size after conv layers\n",
    "        # For 224x224 input: 224 -> 112 -> 56 -> 28 after 3 maxpool layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 28 * 28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x should already be a tensor in [0, 1] range\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca752b2",
   "metadata": {},
   "source": [
    "DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "904cf530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ Loading dataset...\n",
      "  Folder 'dataset' not found. Trying CSV-based loading...\n",
      "  Using CSV: train_clean.csv\n",
      "  Using image directory: Combined_Resized_256\n",
      "‚úÖ Loaded 5701 images from CSV\n",
      "üìä Train samples: 4560, Validation samples: 1141\n"
     ]
    }
   ],
   "source": [
    "# Try to load from folder structure first, then fall back to CSV\n",
    "USE_CSV = False\n",
    "\n",
    "print(f\"\\nüìÇ Loading dataset...\")\n",
    "if os.path.exists(DATASET_PATH):\n",
    "    print(f\"  Found folder structure: {DATASET_PATH}\")\n",
    "    full_dataset = ImageFolderDataset(DATASET_PATH, transform=preprocessing_transform)\n",
    "    print(f\"‚úÖ Loaded {len(full_dataset)} images from folder structure\")\n",
    "else:\n",
    "    # Try CSV-based loading\n",
    "    print(f\"  Folder '{DATASET_PATH}' not found. Trying CSV-based loading...\")\n",
    "    if os.path.exists(CSV_PATH):\n",
    "        # If IMAGE_DIR is not set, try to find it automatically\n",
    "        if IMAGE_DIR is None:\n",
    "            # Try common image directories in current folder\n",
    "            possible_dirs = [\n",
    "                \"Combined_Resized_256\",\n",
    "                \"Normalised_Image_256\", \n",
    "                \"Standardized_Image_256\",\n",
    "                \"Renamed_Ok\",\n",
    "                \"Renamed_Not_OK\"\n",
    "            ]\n",
    "            \n",
    "            for img_dir in possible_dirs:\n",
    "                if os.path.exists(img_dir):\n",
    "                    IMAGE_DIR = img_dir\n",
    "                    break\n",
    "        \n",
    "        if IMAGE_DIR is None or not os.path.exists(IMAGE_DIR):\n",
    "            print(f\"\\n‚ùå Error: Image directory not found!\")\n",
    "            print(f\"   Please set IMAGE_DIR in export.py (around line 19)\")\n",
    "            print(f\"   Example: IMAGE_DIR = r'C:\\\\path\\\\to\\\\your\\\\images'\")\n",
    "            print(f\"\\n   Or create a 'dataset' folder with this structure:\")\n",
    "            print(f\"     dataset/\")\n",
    "            print(f\"       ‚îú‚îÄ‚îÄ 0/  (Non-defective images)\")\n",
    "            print(f\"       ‚îî‚îÄ‚îÄ 1/  (Defective images)\")\n",
    "            exit(1)\n",
    "        \n",
    "        print(f\"  Using CSV: {CSV_PATH}\")\n",
    "        print(f\"  Using image directory: {IMAGE_DIR}\")\n",
    "        full_dataset = CSVImageDataset(CSV_PATH, IMAGE_DIR, transform=preprocessing_transform)\n",
    "        USE_CSV = True\n",
    "        print(f\"‚úÖ Loaded {len(full_dataset)} images from CSV\")\n",
    "    else:\n",
    "        print(f\"‚ùå Error: Neither dataset folder '{DATASET_PATH}' nor CSV file '{CSV_PATH}' found!\")\n",
    "        print(\"Please either:\")\n",
    "        print(\"  1. Create a dataset folder with structure: dataset/0/ and dataset/1/\")\n",
    "        print(\"  2. Or provide train_clean.csv and set IMAGE_DIR in the script\")\n",
    "        exit(1)\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], \n",
    "                                         generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"üìä Train samples: {train_size}, Validation samples: {val_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41375ee",
   "metadata": {},
   "source": [
    "CREATE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "444aee88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Model Architecture:\n",
      "CNNModel(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=100352, out_features=128, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = CNNModel(num_classes=NUM_CLASSES).to(DEVICE)\n",
    "print(\"\\nüìã Model Architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d273a3",
   "metadata": {},
   "source": [
    "COMPILE MODEL (Loss & Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46c3b828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Model compiled with CrossEntropyLoss and Adam optimizer\n"
     ]
    }
   ],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()  # For multi-class (equivalent to sparse_categorical_crossentropy)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "print(f\"\\n‚úÖ Model compiled with CrossEntropyLoss and Adam optimizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9d96c2",
   "metadata": {},
   "source": [
    "TRAIN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6af14659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Starting training for 12 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/12 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 143/143 [01:42<00:00,  1.39it/s, loss=0.4939, acc=81.67%]\n",
      "Epoch 1/12 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:11<00:00,  3.11it/s, loss=0.1915, acc=83.52%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/12:\n",
      "  Train Loss: 0.4660, Train Acc: 81.67%\n",
      "  Val Loss: 0.4186, Val Acc: 83.52%\n",
      "  üíæ Saved best model (Val Acc: 83.52%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/12 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 143/143 [01:39<00:00,  1.44it/s, loss=0.4505, acc=82.21%]\n",
      "Epoch 2/12 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:11<00:00,  3.10it/s, loss=0.2363, acc=83.79%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/12:\n",
      "  Train Loss: 0.4315, Train Acc: 82.21%\n",
      "  Val Loss: 0.3962, Val Acc: 83.79%\n",
      "  üíæ Saved best model (Val Acc: 83.79%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/12 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 143/143 [01:33<00:00,  1.53it/s, loss=0.6184, acc=82.30%]\n",
      "Epoch 3/12 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:18<00:00,  1.94it/s, loss=0.2193, acc=83.61%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/12:\n",
      "  Train Loss: 0.4164, Train Acc: 82.30%\n",
      "  Val Loss: 0.3852, Val Acc: 83.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/12 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 143/143 [01:36<00:00,  1.48it/s, loss=0.3091, acc=82.65%]\n",
      "Epoch 4/12 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:11<00:00,  3.21it/s, loss=0.2175, acc=83.61%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/12:\n",
      "  Train Loss: 0.4079, Train Acc: 82.65%\n",
      "  Val Loss: 0.3826, Val Acc: 83.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/12 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 143/143 [01:36<00:00,  1.48it/s, loss=0.3526, acc=82.35%]\n",
      "Epoch 5/12 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:11<00:00,  3.19it/s, loss=0.2595, acc=83.61%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/12:\n",
      "  Train Loss: 0.4023, Train Acc: 82.35%\n",
      "  Val Loss: 0.3925, Val Acc: 83.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/12 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 143/143 [01:38<00:00,  1.44it/s, loss=0.3967, acc=82.83%]\n",
      "Epoch 6/12 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:11<00:00,  3.20it/s, loss=0.2882, acc=83.79%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/12:\n",
      "  Train Loss: 0.3928, Train Acc: 82.83%\n",
      "  Val Loss: 0.3864, Val Acc: 83.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/12 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 143/143 [01:37<00:00,  1.47it/s, loss=0.1703, acc=82.52%]\n",
      "Epoch 7/12 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:11<00:00,  3.16it/s, loss=0.1877, acc=83.52%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/12:\n",
      "  Train Loss: 0.3899, Train Acc: 82.52%\n",
      "  Val Loss: 0.3650, Val Acc: 83.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/12 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 143/143 [01:36<00:00,  1.47it/s, loss=0.3484, acc=82.72%]\n",
      "Epoch 8/12 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:11<00:00,  3.22it/s, loss=0.2467, acc=84.66%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/12:\n",
      "  Train Loss: 0.3778, Train Acc: 82.72%\n",
      "  Val Loss: 0.3570, Val Acc: 84.66%\n",
      "  üíæ Saved best model (Val Acc: 84.66%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/12 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 143/143 [01:35<00:00,  1.50it/s, loss=0.3687, acc=83.49%]\n",
      "Epoch 9/12 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:11<00:00,  3.22it/s, loss=0.2816, acc=85.10%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/12:\n",
      "  Train Loss: 0.3666, Train Acc: 83.49%\n",
      "  Val Loss: 0.3599, Val Acc: 85.10%\n",
      "  üíæ Saved best model (Val Acc: 85.10%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/12 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 143/143 [01:38<00:00,  1.45it/s, loss=0.2094, acc=84.98%]\n",
      "Epoch 10/12 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:11<00:00,  3.02it/s, loss=0.2548, acc=85.63%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/12:\n",
      "  Train Loss: 0.3454, Train Acc: 84.98%\n",
      "  Val Loss: 0.3322, Val Acc: 85.63%\n",
      "  üíæ Saved best model (Val Acc: 85.63%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/12 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 143/143 [01:41<00:00,  1.41it/s, loss=0.0427, acc=86.64%]\n",
      "Epoch 11/12 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:12<00:00,  2.91it/s, loss=0.1181, acc=87.99%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/12:\n",
      "  Train Loss: 0.3133, Train Acc: 86.64%\n",
      "  Val Loss: 0.2602, Val Acc: 87.99%\n",
      "  üíæ Saved best model (Val Acc: 87.99%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/12 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 143/143 [01:42<00:00,  1.39it/s, loss=0.0879, acc=91.49%]\n",
      "Epoch 12/12 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:12<00:00,  2.85it/s, loss=0.0598, acc=91.67%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/12:\n",
      "  Train Loss: 0.2043, Train Acc: 91.49%\n",
      "  Val Loss: 0.1747, Val Acc: 91.67%\n",
      "  üíæ Saved best model (Val Acc: 91.67%)\n",
      "\n",
      "‚úÖ Training completed! Best validation accuracy: 91.67%\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nüöÄ Starting training for {EPOCHS} epochs...\")\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\")\n",
    "    for images, labels in train_pbar:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{100 * train_correct / train_total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    train_acc = 100 * train_correct / train_total\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]\")\n",
    "        for images, labels in val_pbar:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            val_pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{100 * val_correct / val_total:.2f}%'\n",
    "            })\n",
    "    \n",
    "    val_acc = 100 * val_correct / val_total\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}:\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'val_acc': val_acc,\n",
    "            'model_config': {'num_classes': NUM_CLASSES, 'img_size': IMG_SIZE}\n",
    "        }, MODEL_SAVE_PATH)\n",
    "        print(f\"  üíæ Saved best model (Val Acc: {val_acc:.2f}%)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Training completed! Best validation accuracy: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13961a7",
   "metadata": {},
   "source": [
    "EXPORT MODEL (PIPELINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4513dfae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Model exported successfully at: cnn_pipeline_model.pth\n",
      "‚úÖ Complete model (with architecture) saved at: cnn_pipeline_model_complete.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the best model for export\n",
    "checkpoint = torch.load(MODEL_SAVE_PATH, map_location=DEVICE)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Save the complete model (including architecture)\n",
    "torch.save(model, MODEL_SAVE_PATH.replace('.pth', '_complete.pth'))\n",
    "print(f\"\\n‚úÖ Model exported successfully at: {MODEL_SAVE_PATH}\")\n",
    "print(f\"‚úÖ Complete model (with architecture) saved at: {MODEL_SAVE_PATH.replace('.pth', '_complete.pth')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073d5bf0",
   "metadata": {},
   "source": [
    "LOAD MODEL (FOR TESTING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d6514ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Testing model loading...\n",
      "‚úÖ Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nüîÑ Testing model loading...\")\n",
    "loaded_model = torch.load(\n",
    "    MODEL_SAVE_PATH.replace('.pth', '_complete.pth'),\n",
    "    map_location=DEVICE,\n",
    "    weights_only=False   # ‚úÖ REQUIRED in PyTorch 2.6+\n",
    ")\n",
    "loaded_model.eval()\n",
    "print(\"‚úÖ Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead74546",
   "metadata": {},
   "source": [
    "SAMPLE INFERENCE FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10e4582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_image(image_path, model_path=None):\n",
    "    \"\"\"\n",
    "    Predict a single image using the exported model.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the image file\n",
    "        model_path: Path to the saved model (default: uses the exported model)\n",
    "    \n",
    "    Returns:\n",
    "        predicted_class: Class index (0 or 1)\n",
    "        confidence: Confidence score\n",
    "    \"\"\"\n",
    "    if model_path is None:\n",
    "        model_path = MODEL_SAVE_PATH.replace('.pth', '_complete.pth')\n",
    "    \n",
    "    # Load model\n",
    "    model = torch.load(model_path, map_location=DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img_tensor = preprocessing_transform(img).unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img_tensor)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "        confidence = probabilities[0][predicted_class].item()\n",
    "    \n",
    "    return predicted_class, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf308117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_image(image_path, model_path=None):\n",
    "    if model_path is None:\n",
    "        model_path = MODEL_SAVE_PATH\n",
    "\n",
    "    checkpoint = torch.load(model_path, map_location=DEVICE)\n",
    "\n",
    "    model = CNNModel(num_classes=NUM_CLASSES).to(DEVICE)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img_tensor = preprocessing_transform(img).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img_tensor)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "        confidence = probabilities[0][predicted_class].item()\n",
    "\n",
    "    return predicted_class, confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a5fe6e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "‚úÖ Export pipeline completed successfully!\n",
      "============================================================\n",
      "\n",
      "üìù Usage:\n",
      "  To use the exported model:\n",
      "    model = torch.load('cnn_pipeline_model_complete.pth')\n",
      "    model.eval()\n",
      "\n",
      "  Or use the predict function:\n",
      "    class_id, confidence = predict_single_image('path/to/image.jpg')\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Export pipeline completed successfully!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìù Usage:\")\n",
    "print(f\"  To use the exported model:\")\n",
    "print(f\"    model = torch.load('{MODEL_SAVE_PATH.replace('.pth', '_complete.pth')}')\")\n",
    "print(f\"    model.eval()\")\n",
    "print(f\"\\n  Or use the predict function:\")\n",
    "print(f\"    class_id, confidence = predict_single_image('path/to/image.jpg')\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
